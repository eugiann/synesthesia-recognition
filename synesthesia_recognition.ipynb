{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 0) Importazione librerie\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "FMLz2s0Rv82U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgBzkhGPkitT"
      },
      "outputs": [],
      "source": [
        "# 1) Read file and create list of sentences\n",
        "\n",
        "# split di ukWak pari al 10% del totale (dimensione file 1 GB circa)\n",
        "eng_corpus = \"/content/drive/MyDrive/eng_corpus_ukWac2.txt\"  \n",
        "sentences = []\n",
        "\n",
        "with open(eng_corpus, encoding = \"ISO-8859-1\") as file:\n",
        "  count = sum(1 for _ in file)\n",
        "  \n",
        "with open(eng_corpus, encoding = \"ISO-8859-1\") as file:\n",
        "  for i, line in enumerate(file):\n",
        "    if (i%2 !=0):  # salta le righe dispari che contengono i link alle frasi\n",
        "      continue\n",
        "    if (i<=count):\n",
        "      for l in sent_tokenize(line):\n",
        "        sentences.append(l)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "print(type(sentences))\n",
        "print(len(sentences))\n",
        "print(sentences[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Reading Sensicon 1.0.0 dictionary\n",
        "\n",
        "sensicon_path='/content/drive/MyDrive/Sensicon/sensicon1.0.0/Sensicon1.0.0.txt'\n",
        "sensicon = pd.read_csv(sensicon_path, sep='\\t', header=None)\n",
        "sensicon.columns= ['lemma', 'Sight', 'Hearing', 'Taste', 'Smell', 'Touch']\n",
        "lemmas = [x.split('__') for x in sensicon['lemma'].values]\n",
        "sensicon['pos'] = [lemma[1] for lemma in lemmas]\n",
        "sensicon['lemma'] = [lemma[0] for lemma in lemmas]\n"
      ],
      "metadata": {
        "id": "NIM_r8Q0lir_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Class SensiconWord definition\n",
        "\n",
        "class SensiconWord:\n",
        "  def __init__(self, lemma, sight, hearing, taste, smell, touch, is_relevant, pos, position):\n",
        "    self.lemma=lemma\n",
        "    self.sight=sight\n",
        "    self.hearing=hearing\n",
        "    self.taste=taste\n",
        "    self.smell=smell\n",
        "    self.touch=touch\n",
        "    self.is_relevant=is_relevant\n",
        "    self.pos=pos\n",
        "    self.position=position\n",
        "\n",
        "\n",
        "# 4) Create list of SensiconWord objects (one for each word in the dictionary)\n",
        "\n",
        "sensicon_word_list = []\n",
        "with open(sensicon_path) as file1:\n",
        "    lines = file1.readlines()\n",
        "    for index, line in enumerate(lines):\n",
        "       wlist = line.split();\n",
        "       s = SensiconWord(wlist[0].split('__')[0], wlist[1], wlist[2], \n",
        "                        wlist[3], wlist[4], wlist[5], False, wlist[0].split('__')[1], 0)\n",
        "       sensicon_word_list.append(s)    \n",
        "\n",
        "print(len(sensicon_word_list)) \n"
      ],
      "metadata": {
        "id": "0onz-LjFmTTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Create and populate dictionaries (one for each POS, to avoid duplicates) for nouns (n), adjectives (a), verbs (v) and adverbs (r)\n",
        "import json\n",
        "\n",
        "sense_dictionary_nouns = {}\n",
        "sense_dictionary_adjs = {}\n",
        "sense_dictionary_verbs = {}\n",
        "sense_dictionary_advs = {}\n",
        "\n",
        "for i,s in enumerate(sensicon_word_list):\n",
        "  if sensicon_word_list[i].pos == 'n':\n",
        "    sense_dictionary_nouns[s.lemma] = {}\n",
        "    sense_dictionary_nouns[s.lemma]['sight'] = sensicon_word_list[i].sight\n",
        "    sense_dictionary_nouns[s.lemma]['hearing'] = sensicon_word_list[i].hearing\n",
        "    sense_dictionary_nouns[s.lemma]['taste'] = sensicon_word_list[i].taste\n",
        "    sense_dictionary_nouns[s.lemma]['smell'] = sensicon_word_list[i].smell\n",
        "    sense_dictionary_nouns[s.lemma]['touch'] = sensicon_word_list[i].touch\n",
        "  elif sensicon_word_list[i].pos == 'a':\n",
        "    sense_dictionary_adjs[s.lemma] = {}\n",
        "    sense_dictionary_adjs[s.lemma]['sight'] = sensicon_word_list[i].sight\n",
        "    sense_dictionary_adjs[s.lemma]['hearing'] = sensicon_word_list[i].hearing\n",
        "    sense_dictionary_adjs[s.lemma]['taste'] = sensicon_word_list[i].taste\n",
        "    sense_dictionary_adjs[s.lemma]['smell'] = sensicon_word_list[i].smell\n",
        "    sense_dictionary_adjs[s.lemma]['touch'] = sensicon_word_list[i].touch\n",
        "  elif sensicon_word_list[i].pos == 'v':\n",
        "    sense_dictionary_verbs[s.lemma] = {}\n",
        "    sense_dictionary_verbs[s.lemma]['sight'] = sensicon_word_list[i].sight\n",
        "    sense_dictionary_verbs[s.lemma]['hearing'] = sensicon_word_list[i].hearing\n",
        "    sense_dictionary_verbs[s.lemma]['taste'] = sensicon_word_list[i].taste\n",
        "    sense_dictionary_verbs[s.lemma]['smell'] = sensicon_word_list[i].smell\n",
        "    sense_dictionary_verbs[s.lemma]['touch'] = sensicon_word_list[i].touch\n",
        "  elif sensicon_word_list[i].pos == 'r':\n",
        "    sense_dictionary_advs[s.lemma] = {}\n",
        "    sense_dictionary_advs[s.lemma]['sight'] = sensicon_word_list[i].sight\n",
        "    sense_dictionary_advs[s.lemma]['hearing'] = sensicon_word_list[i].hearing\n",
        "    sense_dictionary_advs[s.lemma]['taste'] = sensicon_word_list[i].taste\n",
        "    sense_dictionary_advs[s.lemma]['smell'] = sensicon_word_list[i].smell\n",
        "    sense_dictionary_advs[s.lemma]['touch'] = sensicon_word_list[i].touch\n",
        " \n",
        "print(len(sense_dictionary_nouns)) #11083\n",
        "print(len(sense_dictionary_adjs)) #6721\n",
        "print(len(sense_dictionary_verbs)) #3738\n",
        "print(len(sense_dictionary_advs)) #1142\n",
        "#print(json.dumps(sense_dictionary_advs, indent=4))\n"
      ],
      "metadata": {
        "id": "yGZouAvyvCO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 6) Functions to check if a sentence could contain sinesthesia \n",
        "\n",
        "threshold = 0.05\n",
        "\n",
        "def is_sight_lexeme(token, dictionary):\n",
        "    dictionary = dictionary.get(token.strip(), {})\n",
        "    if not bool(dictionary):\n",
        "       return False\n",
        "    if float(dictionary.get('sight')) > threshold:\n",
        "       return True\n",
        "    else:\n",
        "       return False\n",
        "\n",
        "# function to check if a token is a hearing lexeme\n",
        "def is_hearing_lexeme(token, dictionary):\n",
        "    dictionary = dictionary.get(token.strip(), {})\n",
        "    if not bool(dictionary):\n",
        "       return False\n",
        "    if float(dictionary.get('hearing')) > threshold: \n",
        "       return True\n",
        "    else:\n",
        "       return False\n",
        "\n",
        "def is_smell_lexeme(token, dictionary):\n",
        "    dictionary = dictionary.get(token.strip(), {})\n",
        "    if not bool(dictionary):\n",
        "       return False\n",
        "    if float(dictionary.get('smell')) > threshold: \n",
        "       return True\n",
        "    else:\n",
        "       return False\n",
        "\n",
        "def is_touch_lexeme(token, dictionary):\n",
        "    dictionary = dictionary.get(token.strip(), {})\n",
        "    if not bool(dictionary):\n",
        "       return False\n",
        "    if float(dictionary.get('touch')) > threshold: \n",
        "       return True\n",
        "    else:\n",
        "       return False\n",
        "\n",
        "def is_taste_lexeme(token, dictionary):\n",
        "    dictionary = dictionary.get(token.strip(), {})\n",
        "    if not bool(dictionary):\n",
        "       return False\n",
        "    if float(dictionary.get('taste')) > threshold: \n",
        "       return True\n",
        "    else:\n",
        "       return False\n",
        "\n",
        "# function to check if a sentence contains lexemes from both modalities\n",
        "def check_sentence(sentence, log):\n",
        "    sight_found = False\n",
        "    hearing_found = False\n",
        "    smell_found = False\n",
        "    taste_found = False\n",
        "    touch_found = False\n",
        "    for tagged_token in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
        "      #if log: print(tagged_token)\n",
        "      word, pos = tagged_token[0], tagged_token[1]\n",
        "      if pos.startswith('N'):\n",
        "          if log: print(\"Word: \" + word + \". POS: \" + pos)\n",
        "          if is_sight_lexeme(word, sense_dictionary_nouns): sight_found = True\n",
        "          if is_hearing_lexeme(word, sense_dictionary_nouns): hearing_found = True\n",
        "          if is_touch_lexeme(word, sense_dictionary_nouns): touch_found = True\n",
        "          if is_taste_lexeme(word, sense_dictionary_nouns): taste_found = True\n",
        "          if is_smell_lexeme(word, sense_dictionary_nouns): smell_found = True\n",
        "      elif pos.startswith('J'):\n",
        "          if log: print(\"Word: \" + word + \". POS: \" + pos)\n",
        "          if is_sight_lexeme(word, sense_dictionary_adjs): sight_found = True\n",
        "          if is_hearing_lexeme(word, sense_dictionary_adjs): hearing_found = True\n",
        "          if is_touch_lexeme(word, sense_dictionary_adjs): touch_found = True\n",
        "          if is_taste_lexeme(word, sense_dictionary_adjs): taste_found = True\n",
        "          if is_smell_lexeme(word, sense_dictionary_adjs): smell_found = True\n",
        "      elif pos.startswith('V'):\n",
        "          if log: print(\"Word: \" + word + \". POS: \" + pos)\n",
        "          if is_sight_lexeme(word, sense_dictionary_verbs): sight_found = True\n",
        "          if is_hearing_lexeme(word, sense_dictionary_verbs): hearing_found = True\n",
        "          if is_touch_lexeme(word, sense_dictionary_verbs): touch_found = True\n",
        "          if is_taste_lexeme(word, sense_dictionary_verbs): taste_found = True\n",
        "          if is_smell_lexeme(word, sense_dictionary_verbs): smell_found = True\n",
        "      elif pos.startswith('R'):\n",
        "          if log: print(\"Word: \" + word + \". POS: \" + pos)\n",
        "          if is_sight_lexeme(word, sense_dictionary_advs): sight_found = True\n",
        "          if is_hearing_lexeme(word, sense_dictionary_advs): hearing_found = True\n",
        "          if is_touch_lexeme(word, sense_dictionary_advs): touch_found = True\n",
        "          if is_taste_lexeme(word, sense_dictionary_advs): taste_found = True\n",
        "          if is_smell_lexeme(word, sense_dictionary_advs): smell_found = True\n",
        "    return is_sentence_relevant(sight_found, hearing_found, touch_found, taste_found, smell_found)\n",
        "\n",
        "\n",
        "def is_sentence_relevant(sight_found, hearing_found, touch_found, taste_found, smell_found):\n",
        "  res = False\n",
        "  if (sight_found and hearing_found) or (sight_found and touch_found) or (sight_found and taste_found) or (sight_found and smell_found): \n",
        "    res = True\n",
        "  if (hearing_found and touch_found) or (hearing_found and taste_found) or (hearing_found and smell_found): \n",
        "    res = True\n",
        "  if (touch_found and taste_found) or (touch_found and smell_found):\n",
        "    res = True\n",
        "  if (taste_found and smell_found):\n",
        "    res = True\n",
        "  return res;\n",
        "\n",
        "\n",
        "def sense_related_lexemes_from_sentence(sentence, log):\n",
        "   sense_lexemes = []\n",
        "   position=1\n",
        "   for tagged_token in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
        "     word = tagged_token[0]\n",
        "     pos = tagged_token[1]\n",
        "     if log: print(str(position) + \" \" +pos)\n",
        "     s = SensiconWord(word, 0.0, 0.0, 0.0, 0.0, 0.0, False, pos, position)\n",
        "     position+=1;          #lemma, sight, hearing, taste, smell, touch, is_relevant, pos\n",
        "     if pos.startswith('N'):\n",
        "        check_sense_related_word(word, sense_dictionary_nouns, s)\n",
        "     elif pos.startswith('J'):\n",
        "        check_sense_related_word(word, sense_dictionary_adjs, s)\n",
        "     elif pos.startswith('V'):\n",
        "        check_sense_related_word(word, sense_dictionary_verbs, s)  \n",
        "     elif pos.startswith('R'):\n",
        "        check_sense_related_word(word, sense_dictionary_advs, s)\n",
        "     sense_lexemes.append(s)\n",
        "   return sense_lexemes\n",
        "\n",
        "\n",
        "def check_sense_related_word(word, dictionary, sensicon_word):\n",
        "   d = dictionary.get(word.strip(), {})\n",
        "   if is_hearing_lexeme(word, dictionary): \n",
        "     sensicon_word.hearing = float(d.get('hearing'))\n",
        "     sensicon_word.is_relevant = True\n",
        "   if is_touch_lexeme(word, dictionary): \n",
        "     sensicon_word.touch = float(d.get('touch'))\n",
        "     sensicon_word.is_relevant = True\n",
        "   if is_smell_lexeme(word, dictionary): \n",
        "     sensicon_word.smell = float(d.get('smell'))\n",
        "     sensicon_word.is_relevant = True\n",
        "   if is_sight_lexeme(word, dictionary): \n",
        "     sensicon_word.sight = float(d.get('sight'))\n",
        "     sensicon_word.is_relevant = True\n",
        "   if is_taste_lexeme(word, dictionary): \n",
        "     sensicon_word.taste = float(d.get('taste'))\n",
        "     sensicon_word.is_relevant = True\n",
        "\n",
        "def check_if_word_is_in_sensicon(word):\n",
        "  is_present = False\n",
        "  for s in sensicon_word_list:\n",
        "    if s.lemma == word: is_present = True\n",
        "  return is_present\n",
        "\n",
        "#def relevant_words_from_sentence()\n"
      ],
      "metadata": {
        "id": "bm0gwIzlxqdD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Test check sentence function\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "sentence=\"If you smell a burning smell it usually means the belt if off the spinner.\"\n",
        "print(\"SENTENCE:\")\n",
        "print(sentence+\"\\n\")\n",
        "print(\"POS Tagging:\")\n",
        "#print(check_sentence(\"the silence that dwells in the forest is not so black\", True))\n",
        "print(check_sentence(\"The opportunity to see and hear the music of these legendary American stars is a rare one, and not an event that should be missed lightly.\", True))\n",
        "# print(check_sentence(\"the silence that dwells in the forest is not so black\", True))\n",
        "# print(is_sight_lexeme(\"mauve\", sense_dictionary_nouns))\n",
        "# print(is_hearing_lexeme(\"sound\", sense_dictionary_verbs))\n",
        "#print(sentences[:10])\n",
        "#print(sense_related_lexemes_from_sentence(sentence))\n",
        "\n"
      ],
      "metadata": {
        "id": "gyb_9pSWmt8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Extract relevant sentences from corpus\n",
        "\n",
        "relevant_sentences = [sentence for sentence in sentences[:10000] if check_sentence(sentence, False)]\n",
        "non_relevant_sentences = [sentence for sentence in sentences[:10000] if not check_sentence(sentence, False)]\n",
        "print(len(relevant_sentences))  #255\n",
        "print(len(non_relevant_sentences))  #9745\n"
      ],
      "metadata": {
        "id": "B2UXqjwzlzer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Verified metaphors from  excel\n",
        "\n",
        "import json\n",
        "sinestesie_UKWAC_path='/content/drive/MyDrive/sinestesie_UKWAC_2.xlsx'\n",
        "sinestesie_UKWAC = pd.read_excel(sinestesie_UKWAC_path, sheet_name='Foglio3')\n",
        "sinestesie_UKWAC.columns= ['sentence', 'source_s', 'target_s', 'dependency_type', 'syntax', 'source_w', 'target_w', 'comment', 'corpus']\n",
        "\n",
        "sinestesie_UKWAC_list = []\n",
        "for i, s in enumerate(sinestesie_UKWAC['sentence']):\n",
        "  sinestesie_UKWAC_list.append(s[7:])\n",
        "\n",
        "output = {}\n",
        "true_counter=0\n",
        "false_counter=0\n",
        "for i in range(len(sinestesie_UKWAC_list)):\n",
        "  if check_sentence(sinestesie_UKWAC_list[i], False):\n",
        "    print(\"TRUE \" + sinestesie_UKWAC_list[i])\n",
        "    output[i] = {}\n",
        "    output[i]['sentence'] = sinestesie_UKWAC_list[i]\n",
        "    output[i]['relevant'] = True\n",
        "    true_counter+=1\n",
        "  else:\n",
        "    print(\"FALSE \" + sinestesie_UKWAC_list[i])\n",
        "    output[i] = {}\n",
        "    output[i]['sentence'] = sinestesie_UKWAC_list[i]\n",
        "    output[i]['relevant'] = False\n",
        "    false_counter+=1\n",
        "\n",
        "#print(json.dumps(output, indent=4))\n",
        "print(true_counter)\n",
        "print(false_counter)\n"
      ],
      "metadata": {
        "id": "v027ia76Gnqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10) Test if a sentence is recognized as relevant correctly\n",
        "\n",
        "for i, sentence in enumerate(sinestesie_UKWAC_list):\n",
        "  list_sensic = sense_related_lexemes_from_sentence(sentence, False)\n",
        "  print(str(i+1) + \") \"+ sentence)\n",
        "  print(\"Verified synesthetic metaphor:\")\n",
        "  print(\"Source word: \" + sinestesie_UKWAC['source_w'][i], \"(\" + sinestesie_UKWAC['source_s'][i] +\")\")\n",
        "  print(\"Target word: \" + sinestesie_UKWAC['target_w'][i], \"(\" + sinestesie_UKWAC['target_s'][i] +\")\")\n",
        "  print(\"Syntax: \" + sinestesie_UKWAC['syntax'][i])\n",
        "  print(\"Dependency type: \" + sinestesie_UKWAC['dependency_type'][i])\n",
        "  \n",
        "  print(\"\\nSensicon analysis. Relevant words for sentence \" + str(i+1))\n",
        "  for s in list_sensic:\n",
        "    if s.is_relevant:\n",
        "      print(\"(at position \" + str(s.position) +\")\", s.lemma, s.pos, \n",
        "            \"Vision (\" + str(s.sight) + \")\" if (s.sight !=0) else \"\", \n",
        "            \"Hearing (\" + str(s.hearing) + \")\" if (s.hearing !=0) else \"\", \n",
        "            \"Taste (\" +str(s.taste) + \")\" if (s.taste !=0) else \"\", \n",
        "            \"Smell (\" + str(s.smell) + \")\" if (s.smell !=0) else \"\", \n",
        "            \"Touch (\" + str(s.touch) + \")\" if (s.touch !=0) else \"\")\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "DEnsJZ7qX65V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11) POS tagging test\n",
        "\n",
        "#sense_related_lexemes_from_sentence(\"The songs are wonderful, both awkward and hilarious, and Dan 's voice is something else, so low and warm.\", True)\n",
        "\n",
        "nltk.pos_tag(nltk.word_tokenize(\"The songs are wonderful, both awkward and hilarious, and Dan 's voice is something else, so low and warm.\"))"
      ],
      "metadata": {
        "id": "OhPS_Y0u8AB6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}